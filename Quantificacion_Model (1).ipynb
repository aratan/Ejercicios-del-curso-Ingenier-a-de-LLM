{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "iihpplGcXDwj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "id": "B9MSeEcWU5Qe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2826f570-aeac-482c-f8ef-b6e260fe5176"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Jan 15 12:18:14 2025       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   57C    P0              29W /  70W |   1493MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip cache purge\n",
        "!pip install --upgrade pip  # Upgrade pip to the latest version\n",
        "!pip install -q huggingface_hub  # Install using the new package name\n",
        "!pip install -q huggingface-cli\n",
        "!pip install -q transformers accelerate bitsandbytes\n",
        "!pip install -q torch torchvision torchaudio\n",
        "!pip install -q tiktoken requests sentencepiece\n",
        "!pip install -q transformers_stream_generator # Install the missing package\n",
        "!pip install -q requests\n",
        "!pip install -q huggingface_hub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcrmhNGsXDOt",
        "outputId": "d071da1b-2b8e-4b76-b1a0-40ab7212e32a"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files removed: 2\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (24.3.1)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement huggingface-cli (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for huggingface-cli\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "from huggingface_hub import login\n",
        "# Retrieve your Hugging Face token from Colab secrets\n",
        "HUGGINGFACE_TOKEN = userdata.get('HF_TOKEN')\n",
        "login(HUGGINGFACE_TOKEN, add_to_git_credential=True)\n",
        "\n",
        "#from huggingface_hub import notebook_login\n",
        "#notebook_login()\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, pipeline\n",
        "import torch\n",
        "from transformers import BitsAndBytesConfig\n",
        "#from bitsandbytes import BitsAndBytesConfig\n",
        "\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "zl5ZRcBEG1yN"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "QWEN = \"Qwen/Qwen2.5-0.5B\"\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"eres un asistente util\"},\n",
        "    {\"role\": \"user\", \"content\": \"Cuentame un chiste de peras\"}\n",
        "]\n"
      ],
      "metadata": {
        "id": "fYdadCiLViJu"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quantificacion la magia es aqui\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    # Use CUDA if available\n",
        "    quantization_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16\n",
        "    )\n",
        "    device_map = \"auto\"  # Use GPU\n",
        "else:\n",
        "    # Fallback to CPU if CUDA is not available\n",
        "    quantization_config = None  # Disable quantization\n",
        "    device_map = \"cpu\"  # Use CPU"
      ],
      "metadata": {
        "id": "uTS5Kw6od1Vf"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "source": [
        "# TOKEN\n",
        "tokenizer = AutoTokenizer.from_pretrained(QWEN)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "# if cuda is available inputs to cuda, otherwise CPU\n",
        "if torch.cuda.is_available():\n",
        "  inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n",
        "else:\n",
        "  inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cpu\")\n",
        "print(inputs)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-jLSs4z49A9",
        "outputId": "5823fa0b-d05a-4ee2-9731-7408c296507c"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[151644,   8948,    198,  12917,    650,    438,    380,   6817,   4094,\n",
            "         151645,    198, 151644,    872,    198,     34,  11680,    373,    650,\n",
            "            521,  16776,    409,    817,    300, 151645,    198]],\n",
            "       device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MODEL descarga modelo y pesos *importante*\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(QWEN, device_map=device_map, quantization_config=quantization_config)\n",
        "\n"
      ],
      "metadata": {
        "id": "t4Rzj-6PgIu3"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory = model.get_memory_footprint() / 1e6\n",
        "print(f\"Peso en Memory RAM del modelo: {memory:,.1f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMLAD7SQpPJW",
        "outputId": "1cf895f9-ce01-4d33-bc67-4c7487930893"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Peso en Memory RAM del modelo: 451.3 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model # capa de entrada vocabulario , numeros: Embedding(151936, 896)\n",
        "# tenemos capa de atencion: Qwen2SdpaAttention, multicapa Qwen2MLP, funcion de activacion: SiLU()\n",
        "# capa de normalizacion:  Qwen2RMSNorm y capa linial  Linear"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0ar1DDPnt9n",
        "outputId": "00bbcd9c-8cfc-4efb-ac99-fdc314539c82"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Qwen2ForCausalLM(\n",
              "  (model): Qwen2Model(\n",
              "    (embed_tokens): Embedding(151936, 896)\n",
              "    (layers): ModuleList(\n",
              "      (0-23): 24 x Qwen2DecoderLayer(\n",
              "        (self_attn): Qwen2SdpaAttention(\n",
              "          (q_proj): Linear4bit(in_features=896, out_features=896, bias=True)\n",
              "          (k_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n",
              "          (v_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n",
              "          (o_proj): Linear4bit(in_features=896, out_features=896, bias=False)\n",
              "          (rotary_emb): Qwen2RotaryEmbedding()\n",
              "        )\n",
              "        (mlp): Qwen2MLP(\n",
              "          (gate_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=4864, out_features=896, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
              "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
              "      )\n",
              "    )\n",
              "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
              "    (rotary_emb): Qwen2RotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate( inputs, max_new_tokens=80 )\n",
        "print(tokenizer.decode(outputs[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcfqOtbMnvOQ",
        "outputId": "8b853959-fed1-4209-d4ff-2542018f5ea8"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|im_start|>system\n",
            "eres un asistente util<|im_end|>\n",
            "<|im_start|>user\n",
            "Cuentame un chiste de peras<|im_end|>\n",
            "Cuentame un chiste de perasacco\n",
            "Cuentame un chiste de perasacco\n",
            "Cuentame un chiste de perasacco\n",
            "Cuentame un chiste de perasacco\n",
            "Cuentame un chiste de perasacco\n",
            "Cuentame un chiste de perasacco\n",
            "Cuentame un chiste de perasacco\n",
            "Cuentame\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Limpiar\n",
        "model.save_pretrained(\"qwen_quantized\")\n",
        "del inputs, outputs, model\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "6lPWmjCZnvYW"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# funcion de generacion y stream quantificado , recuerda que la config la tiene arriba\n",
        "\n",
        "def generate(model, messages):\n",
        "  tokenizer = AutoTokenizer.from_pretrained(QWEN)\n",
        "  tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "  inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n",
        "  streamer = TextStreamer(tokenizer)\n",
        "  model = AutoModelForCausalLM.from_pretrained(QWEN, device_map=\"auto\", quantization_config=quantization_config)\n",
        "  outputs = model.generate( inputs, max_new_tokens=80, streamer=streamer )\n",
        "  # Now you can save the model\n",
        "  model.save_pretrained(\"qwen_quantized\")\n",
        "  del tokenizer, streamer, model, inputs , outputs\n",
        "  torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "id": "Q9WiRsLEsaEu"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "source": [
        "from google.colab import files\n",
        "!zip -r qwen_quantized.zip qwen_quantized\n",
        "# Check if the zip file was created before attempting to download\n",
        "import os\n",
        "if os.path.exists(\"qwen_quantized.zip\"):\n",
        "  files.download('qwen_quantized.zip')\n",
        "else:\n",
        "  print(\"Error: qwen_quantized.zip not found. Make sure the 'generate' function was called and the model was saved successfully.\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "EWkCivnR3P02",
        "outputId": "4ad247c4-6d54-47ca-802a-efbba921da77"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: qwen_quantized/ (stored 0%)\n",
            "  adding: qwen_quantized/model.safetensors (deflated 16%)\n",
            "  adding: qwen_quantized/generation_config.json (deflated 29%)\n",
            "  adding: qwen_quantized/config.json (deflated 55%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_999fd9ee-b457-4a89-ac53-8b3ea58942a2\", \"qwen_quantized.zip\", 385663191)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}