{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe12c203-e6a6-452c-a655-afb8a03a4ff5",
   "metadata": {},
   "source": [
    "# Ejercicio final de la semana 1\n",
    "\n",
    "Para demostrar que estás familiarizado con la API de OpenAI y también con Ollama, crea una herramienta que responda a una pregunta técnica\n",
    "y la explique. ¡Esta es una herramienta que podrás usar durante el curso!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7cfd18ec-ea69-4329-833e-00e3ac294026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Respuesta de llama3 (usando el cliente nativo de Ollama):**\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Vamos a analizar el código step by step.\n",
       "\n",
       "El código que proporcionas es un ejemplo de uso de recursion y yield en Python. Vamos a explicar qué hace cada parte del código.\n",
       "\n",
       "**Nota:** Es importante mencionar que la variable `books` no está definida en este snippet, por lo que asumiremos que se refiere a una lista o algún objeto que contiene información sobre libros (título, autor, etc.).\n",
       "\n",
       "**Parágrafos**\n",
       "\n",
       "La función principal es `(book.get(\"author\") for book in books if book.get(\"author\"))`.\n",
       "\n",
       "Esta expresión se conoce como un `generator expression`. Es una forma compacta de crear un generator.\n",
       "\n",
       "Un generator es una función que devuelve un iterador, en lugar de una lista o otro tipo de colección. Los generators permiten ahorrar memoria y recursos computacionales al no almacenar toda la colección en memoria a la vez.\n",
       "\n",
       "**`book.get(\"author\")`**\n",
       "\n",
       "Esta expresión intenta obtener el valor de la clave `\"author\"` desde el diccionario `book`. Si la clave no existe, devuelve `None`.\n",
       "\n",
       "**`for book in books if book.get(\"author\")`**\n",
       "\n",
       "Este es un filtro para seleccionar solo aquellos diccionarios `book` que tienen una clave `\"author\"` presente. De esta manera, se evita iterar sobre diccionarios vacíos o que carecen de información.\n",
       "\n",
       "**`yield from`**\n",
       "\n",
       "El término `yield from` fue introducido en Python 3.3. Se utiliza para \"pasar el control\" a otro generator. En otras palabras, crea un nuevo generator y lo convierte en su propio sub-generator.\n",
       "\n",
       "En este caso específico, se utiliza `yield from` para \"pasar el control\" al generator interno `(book.get(\"author\") for book in books if book.get(\"author\"))`.\n",
       "\n",
       "**El comportamiento del código**\n",
       "\n",
       "Ahora que hemos explicado cada parte del código, veamos cómo se comporta.\n",
       "\n",
       "1. Se crea un generator que intenta obtener el valor de la clave `\"author\"` desde cada diccionario `book` en la lista `books`.\n",
       "2. El filtro `(for book in books if book.get(\"author\"))` selecciona solo aquellos diccionarios que tienen información sobre autores.\n",
       "3. `yield from` pasa el control al generator interno, permitiendo que se ejecute por sí solo.\n",
       "4. Cada iteración del generator intenta obtener el valor de la clave `\"author\"` y lo devuelve (o `None`, si no existe).\n",
       "5. La función devuelve un iterator iterable sobre estos valores obtenidos.\n",
       "\n",
       "**Por qué este código es útil**\n",
       "\n",
       "Este código puede ser utilizado en varias situaciones:\n",
       "\n",
       "*   Obtener una lista de autores de libros desde una base de datos que tiene información sobre libros y autores.\n",
       "*   Filtrar y procesar información de manera eficiente, sin almacenar toda la información en memoria a la vez.\n",
       "\n",
       "Recuerda que este código se utiliza para obtener un conjunto de valores a partir de otro conjunto, por lo que debe ser tratado con precaución si no se comprenderán las condiciones de la data."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ollama\n",
    "from IPython.display import Markdown, display, update_display\n",
    "\n",
    "MODEL_LLAMA = 'llama3'  # Asegúrate de que este modelo esté disponible en tu instancia de Ollama\n",
    "\n",
    "# Configura el cliente nativo de Ollama\n",
    "ollama_native = ollama.Client(\n",
    "    host='http://localhost:11434'\n",
    ")\n",
    "\n",
    "question = \"\"\"\n",
    "Explica qué hace este código y por qué:\n",
    "yield from (book.get(\"author\") for book in books if book.get(\"author\"))\n",
    "\"\"\"\n",
    "system_prompt = \"Eres un tutor técnico útil que responde preguntas sobre código Python, ingeniería de software, ciencia de datos y LLM\"\n",
    "user_prompt = f\"Por favor, da una explicación detallada de la siguiente pregunta: {question}\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "]\n",
    "\n",
    "print(f\"**Respuesta de {MODEL_LLAMA} (usando el cliente nativo de Ollama):**\")\n",
    "response_ollama_native = ollama_native.chat(\n",
    "    model=MODEL_LLAMA,\n",
    "    messages=messages,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "response = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in response_ollama_native:\n",
    "    response += chunk['message']['content'] or ''\n",
    "    response = response.replace(\"```\", \"\").replace(\"markdown\", \"\")\n",
    "    update_display(Markdown(response), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0a0b8e4a-6873-429d-ba27-d6bb350af424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Respuesta de llama3 (usando el cliente nativo de Ollama):**\n",
      "**Respuesta de gpt-4o-mini (usando la API de OpenAI):**\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Este código utiliza la sintaxis `yield from` en Python para generar una secuencia de autoras de libros (`authors`) a partir de una lista de diccionarios (`books`). Aquí te explico qué hace cada parte:\n",
       "\n",
       "*   `(book.get(\"author\") for book in books if book.get(\"author\"))`: \n",
       "    Este es un **generador** que se crea por la sintaxis `for` y tiene la forma `(expression for variable in iterable if condition)`. Generadores son programas que generan secuencias de resultados a medida que se los pide.\n",
       "\n",
       "    En este caso, se intenta acceder a cada libro (`book`) en la lista (`books`) y si la clave (\"author\") existe (`if book.get(\"author\"`), se devuelve el valor asociado. Se utiliza `.get()` para evitar un error de `KeyError` si no existe una clave en el diccionario.\n",
       "\n",
       "    Por lo tanto, esta secuencia genera un iterable que recorre los libros que contienen la clave (\"author\") y retorna el autor correspondiente.\n",
       "\n",
       "*   `yield from(...)`: Es un operador `yield from`, usado para generar una secuencia de resultados de otro generador. Cualquier llamada a yield `from` genera una secuencia de los valores por yield del generador pasado.\n",
       "\n",
       "En resumen, se usa este código en un contexto de procesamiento de datos, quizá para extraer un conjunto de valores de un conjunto de diccionarios (`books`). \n",
       "\n",
       "Por ejemplo:\n",
       "\n",
       "```python\n",
       "import json\n",
       "\n",
       "# suponiendo que books es una lista de diccionario json\n",
       "books = [\n",
       "    {\"title\": \"Libro1\", \"author\": \"AuthorX\"},\n",
       "    {\"title\": \"Libro2\", \"author\": \"AuthorY\"},\n",
       "    # ...\n",
       "]\n",
       "\n",
       "# obtener las autoras\n",
       "def get_authors():\n",
       "    yield from (book.get(\"author\") for book in books if book.get(\"author\"))\n",
       "\n",
       "# utilizar el generador\n",
       "for author in get_authors():\n",
       "    print(author)\n",
       "```\n",
       "\n",
       "Este código se ejecutaría de la siguiente manera:\n",
       "\n",
       "1.  Se define una lista `books` donde cada elemento es un diccionario que representa un libro con su título y autor.\n",
       "2.  Se define un generador `get_authors()` que utiliza `(book.get(\"author\") for book in books if book.get(\"author\"))` para obtener solo las autoras de los libros que tienen una clave (\"author\") presente en sus diccionarios.\n",
       "3.  El uso de `yield from` significa que en lugar de devolver los valores directamente, se llama al generador interno `(book.get(\"author\") for book in books if book.get(\"author\"))`. Este generador es responsable de generar y devolver la secuencia de autoras según el filtro.\n",
       "4.  Finalmente, en un bucle `for`, se itera sobre los valores del generador obteniendo solo las autoras."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Error de la API de OpenAI: Error code: 401 - {'error': {'message': 'Your authentication token is not from a valid issuer.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_issuer'}}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import ollama\n",
    "import openai\n",
    "from IPython.display import Markdown, display\n",
    "import threading\n",
    "\n",
    "# Cargar variables de entorno desde .env\n",
    "load_dotenv()\n",
    "\n",
    "MODEL_LLAMA = 'llama3'\n",
    "OLLAMA_HOST = 'http://localhost:11434'\n",
    "MODEL_OPENAI = \"gpt-4o-mini\"  # Puedes ajustar el modelo de OpenAI si lo deseas\n",
    "\n",
    "# Obtener la API key de OpenAI desde las variables de entorno\n",
    "OPENAI_API_KEY =\"***tjO8Z6AFbBeTLgRT***\"\n",
    "\n",
    "def obtener_respuesta_ollama(question, system_prompt, model_name=MODEL_LLAMA, host=OLLAMA_HOST):\n",
    "    try:\n",
    "        ollama_native = ollama.Client(host=host)\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ]\n",
    "        response_ollama_native = ollama_native.chat(\n",
    "            model=model_name,\n",
    "            messages=messages,\n",
    "            stream=True\n",
    "        )\n",
    "        response = \"\"\n",
    "        for chunk in response_ollama_native:\n",
    "            response += chunk['message']['content'] or ''\n",
    "        return response\n",
    "    except ollama.exceptions.OllamaError as e:\n",
    "        return f\"Error al interactuar con Ollama: {e}\"\n",
    "    except Exception as e:\n",
    "        return f\"Ocurrió un error inesperado con Ollama: {e}\"\n",
    "\n",
    "def obtener_respuesta_openai(question, system_prompt, model_name=MODEL_OPENAI):\n",
    "    if not OPENAI_API_KEY:\n",
    "        return \"Error: La API key de OpenAI no está configurada en el archivo .env\"\n",
    "    try:\n",
    "        client = openai.OpenAI(api_key=OPENAI_API_KEY)\n",
    "        response_stream = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": question}\n",
    "            ],\n",
    "            stream=True,\n",
    "        )\n",
    "        response = \"\"\n",
    "        for chunk in response_stream:\n",
    "            if chunk.choices:\n",
    "                response += chunk.choices[0].delta.content or \"\"\n",
    "        return response\n",
    "    except openai.APIError as e:\n",
    "        return f\"Error de la API de OpenAI: {e}\"\n",
    "    except Exception as e:\n",
    "        return f\"Ocurrió un error inesperado con OpenAI: {e}\"\n",
    "\n",
    "question = \"\"\"\n",
    "Explica qué hace este código y por qué:\n",
    "yield from (book.get(\"author\") for book in books if book.get(\"author\"))\n",
    "\"\"\"\n",
    "system_prompt = \"Eres un tutor técnico útil que responde preguntas sobre código Python, ingeniería de software, ciencia de datos y LLM\"\n",
    "\n",
    "def run_ollama():\n",
    "    print(f\"**Respuesta de {MODEL_LLAMA} (usando el cliente nativo de Ollama):**\")\n",
    "    global respuesta_ollama\n",
    "    respuesta_ollama = obtener_respuesta_ollama(question, system_prompt)\n",
    "\n",
    "def run_openai():\n",
    "    print(f\"**Respuesta de {MODEL_OPENAI} (usando la API de OpenAI):**\")\n",
    "    global respuesta_openai\n",
    "    respuesta_openai = obtener_respuesta_openai(question, system_prompt)\n",
    "\n",
    "# Crear y ejecutar los hilos en paralelo\n",
    "ollama_thread = threading.Thread(target=run_ollama)\n",
    "openai_thread = threading.Thread(target=run_openai)\n",
    "\n",
    "ollama_thread.start()\n",
    "openai_thread.start()\n",
    "\n",
    "ollama_thread.join()\n",
    "openai_thread.join()\n",
    "\n",
    "# Mostrar las respuestas después de que ambos hilos hayan terminado\n",
    "display(Markdown(respuesta_ollama))\n",
    "display(Markdown(respuesta_openai))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23edd34-fc79-435b-8671-da3932e3bf2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b1b593-98c4-4510-9afc-93cda390da3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
